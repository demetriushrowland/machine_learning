Linear Regression

Say we are given (n+1) data points in (p+1) dimensional space (with p the number of predictors):
    (x01, x02, ..., x0p, y0)
    (x11, x12, ..., x1p, y1)
    ...
    (xn1, xn2, ..., xnp, yn)
    
We wish to find the best linear approximation for these (n+1) points.

The goal is to minimize the error function err(b0,b1,b2,...,bp) = 1/2*((y0-y0_appr)**2 + (y1-y1_appr)**2 + ... + (yn-yn_appr)**2)

Each yi_appr satisfies

    yi_appr = b0 + b1*xi1 + b2*xi2 + ... + bp*xip

with b0, b1, b2, ..., bp the coefficients to be determined.
    
Minimizing the error function amounts to taking the partial derivative of the error function with respect
to each variable b0, b1, b2, ..., bp, setting each expression to 0, and solving for b0 through bp.

We have

d/db0 (err) = (y0-y0_appr)*(-1) + (y1-y1_appr)*(-1) + ... + (yn-yn_appr)*(-1) = 0
              y0 + y1 + ... + yn = y0_appr + y1_appr + ... + yn_appr
              y0 + y1 + ... + yn = b0*[1,1,...,1]·[1,1,...,1] + b1*[x01,x11,x21,...,xn1]·[1,1,...,1] + ... + bp*[x0p,x1p,...,xnp]·[1,1,...,1]
              
d/db1 (err) = (y0-y0_appr)*(-x01) + (y1-y1_appr)*(-x11) + ... + (yn-y1_appr)*(-xn1) = 0
              [y0,y1,...,yn]·[x01,x11,...,xn1] = [y0_appr,y1_appr,...,yn_appr]·[x01,x11,...,xn1]
              [y0,y1,...,yn]·[x01,x11,...,xn1] = [b0+b1*x01+...+bp*x0p,...,b0+b1*xn1+...+bp*xnp]·[x01,x11,...,xn1]
              [y0,y1,...,yn]·[x01,x11,...,xn1] = b0*[1,...,1]·[x01,x11,...,xn1] +...+ bp*[x0p,...,xnp]·[x01,x11,...,xn1]
              
...

d/dbp (err) = [y0,y1,...,yn]·[x0p,x1p,...,xnp] = b0*[1,...,1]·[x0p,x1p,...,xnp] +...+ bp*[x0p,...,xnp]·[x0p,...,xnp]

With a little work, this reduces to the matrix equation (X.T * X) * w = X.T * y

The solution for optimal w is then: w = inv (X.T * X) * X.T * y

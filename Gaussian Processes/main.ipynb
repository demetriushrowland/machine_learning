{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import util\n",
    "from scipy.special import expit\n",
    "from scipy.optimize import root\n",
    "from sklearn import gaussian_process as gp\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting Classification Performance with Gaussian Processes\n",
    "\n",
    "We use the scikit-learn package to implement Gaussian Classification on a selected COVID-19 data set, and we compare performance against benchmark models.\n",
    "\n",
    "**Preliminary Analysis**\n",
    "\n",
    "*Gaussian Processes*\n",
    "\n",
    "First, we implement Gaussian Process Classification on the CT scans of the lungs of hospital patients that fall into three categories:\n",
    "\n",
    "* Normal Patients\n",
    "* Patients with Viral Pneumonia\n",
    "* Patients infected with COVID-19\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = int(10e6)\n",
    "images, class_labels = util.load_covid_images(num_images)\n",
    "NUM_IMAGES = images.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use our custom util module to load all of the scans, of which there are at most *num_images* in each category. We assign a label of \n",
    "\n",
    "* 0 for the patients infected with COVID-19\n",
    "* 1 for the normal patients\n",
    "* 2 for the patients with viral pneumonia\n",
    "\n",
    "and collect these in the variable *class_labels*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 1000\n",
    "pca = PCA(n_components=d)\n",
    "reduced_images = pca.fit_transform(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each image is a vector consisting of 1024 x 1024 x 3 Red/Green/Blue Values. It is computationally intractable to perform GP classification using the image vectors because their dimension is too large. Therefore, we choose to project these vectors into a lower dimensional space using principal component analysis, where we choose d = 1000 principal components. The variable *reduced_images* contains the coordinates of each vector in this lower dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.random.permutation(np.identity(NUM_IMAGES))\n",
    "reduced_images = P @ reduced_images\n",
    "class_labels = P @ class_labels\n",
    "n_train = int(.8*NUM_IMAGES)\n",
    "X_train = reduced_images[:n_train]\n",
    "Y_train = class_labels[:n_train]\n",
    "X_test = reduced_images[n_train:]\n",
    "Y_test = class_labels[n_train:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to ensure a different test set for each replication of the experiment, we randomly permute the reduced image set. We use an 80 / 20 split for the training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianProcessClassifier(copy_X_train=True, kernel=None, max_iter_predict=100,\n",
       "                          multi_class='one_vs_rest', n_jobs=None,\n",
       "                          n_restarts_optimizer=0, optimizer='fmin_l_bfgs_b',\n",
       "                          random_state=None, warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_classifier = gp.GaussianProcessClassifier()\n",
    "gp_classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GP Accuracy: 0.4974182444061962\n"
     ]
    }
   ],
   "source": [
    "print(\"GP Accuracy:\", gp_classifier.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voila! We've trained the GP classifier on the reduced image set and associated class labels. We have specified a zero mean prior and a squared exponential covariance function. The result is a correct prediction on approximately 50% of the test set. Recall that the expected accuracy of guessing randomly would be 33%, since there are three classes. We now train a support vector machine, a feedforward neural network, and a logistic model on the same image set and compare results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Support Vector Machine*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.9466437177280551\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM Accuracy:\", svc.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "*Neural Network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
       "              learning_rate_init=0.001, max_fun=15000, max_iter=200,\n",
       "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
       "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
       "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_classifier = MLPClassifier()\n",
    "mlp_classifier.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Net Accuracy: 0.882960413080895\n"
     ]
    }
   ],
   "source": [
    "print(\"Neural Net Accuracy:\", mlp_classifier.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Logistic Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Zhonghou/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regressor = lm.LogisticRegression()\n",
    "logistic_regressor.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.9053356282271945\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression Accuracy:\", logistic_regressor.score(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results indicate that the performance of GP classification is suboptimal and is beaten by every other model. How can we improve performance? We identify two problems and suggest potential solutions.\n",
    "\n",
    "1. The implementation of GP Classification depends on a Laplace approximation to the posterior distribution of the process f given training set *X_train* and labels *Y_train*, which may not be satisfactory if the posterior is multimodal.\n",
    "\n",
    "2. Our choice of the squared exponential kernel for the GP classifier may not be optimal.\n",
    "\n",
    "For our purposes, we find it sufficient to devote our focus to the second problem, so we explore several other kernels and select the one which maximizes the marginal likelihood of (*X_train, Y_train*) as well as the one that minimizes the PAC generalization error bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Marginal Likelihoood**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.gaussian_process.kernels as kernels\n",
    "model_kernels = [kernels.ConstantKernel(), kernels.Matern(), \n",
    "                 kernels.RationalQuadratic(), kernels.WhiteKernel()]\n",
    "num_kernels = len(model_kernels)\n",
    "test_accuracies = np.empty(num_kernels)\n",
    "log_marginal_likelihoods = np.empty(num_kernels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading four kernels from sklearn's collection and we initialize two arrays to store the test errors for each model and the marginal likelihoods of the training set under each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kernel_num in range(num_kernels):\n",
    "    kernel = model_kernels[kernel_num]\n",
    "    gp_classifier = gp.GaussianProcessClassifier(kernel=kernel)\n",
    "    gp_classifier.fit(X_train, Y_train)\n",
    "    log_marginal_likelihoods[kernel_num] = gp_classifier.log_marginal_likelihood()\n",
    "    test_accuracies[kernel_num] = gp_classifier.score(X_test, Y_test)\n",
    "\n",
    "best_classifier_num = np.argmax(log_marginal_likelihoods)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We iterate over the kernels and store the test errors and marginal likelihoods, and we find the kernel and corresponding classifier that maximizes the marginal likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9397590361445783\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy:\", test_errors[best_classifier_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see this GP Classifier's performance far surpasses that of its predecessor and does about as well as the Support Vector Machine. We now need only compare our results against a different scheme, where we minimize the PAC generalization error bound instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PAC Generalization Error**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_KL(kernel, X, Y):\n",
    "    K = kernel(X, X)\n",
    "    n = K.shape[0]\n",
    "    f0 = np.zeros(n)\n",
    "    def system(f):\n",
    "        pi = expit(f)\n",
    "        return f - K @ (Y - pi)\n",
    "    f_hat = root(system, f0, method='broyden1').x\n",
    "    pi = expit(f_hat)\n",
    "    W = np.identity(n)\n",
    "    for i in range(n):\n",
    "        W[i][i] = pi[i] * (1 - pi[i])\n",
    "    eps = 10e-3\n",
    "    K_inv = la.inv(K + eps*np.identity(n))\n",
    "    A = K_inv + W\n",
    "    kl = 1/2*np.log(abs(la.det(K))+eps)\n",
    "    kl += 1/2*np.log(abs(la.det(A))+eps)\n",
    "    kl += 1/2*np.trace(la.inv(A+eps*np.identity(n))@(K_inv - A))\n",
    "    kl += np.reshape(f_hat, (1, n)) @ K_inv @ f_hat\n",
    "    return kl\n",
    "\n",
    "kl_divergences = np.zeros(num_kernels)\n",
    "for kernel_num in range(num_kernels):\n",
    "    for c in range(3):\n",
    "        Y = (Y_train == c).astype(int)\n",
    "        kl_divergences[kernel_num] += 1/3*compute_KL(kernel, X_train, Y)\n",
    "\n",
    "kl_best_classifier_num = np.argmin(kl_divergences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In accordance with Seeger 2003, the PAC generalization error is bounded by an increasing function of the KL divergence between the prior and posterior distributions over the possible function values at the test and training points. Therefore, if we wish to minimize this error bound, we can minimize the KL divergence. In fact, the KL divergence can be computed by the function above, as noted in Rasmussen and Williams. We compute the KL divergence under each model, and choose the kernel for which it is minimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.44922547332185886\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Accuracy:\", test_accuracies[kl_best_classifier_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a poor test accuracy, as Seeger predicted. In the future, we will heed his advice and use the marginal likelihood, cross validation, or bayesian model selection to distinguish between models, resorting to the PAC bounds only if these other tools are unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
